08/03/21
 * Did lots of work on this today, learned about onehot-encoding vs label encoding etc and that XGB and gradient boosting classifiers are the same thing basically
 * This meant I switched to trying out a decision tree classifier which works very well
 * Wrote a parser for chronic disease and one-hot encoded that
 * Most of the work today was more advanced data-cleaning, largely relating to getting rid of those few strings left and replacing them datetimes/label ids
 * Also dropped the column date_discharge_death as when using watson autoAI this was so stupidly heavily weighted and isn't practical for my question
 * Imputing

 * Need to write a parser for symptoms
 * Look into feature engineering/hyperparam optimization with visualization with seaborn
 * Create a visualisation for feature importance for the final product
 * Used watson autoAI to help me figure out some values for my XGB classifier - look into comparison vs weak values
 * Create a parser for travel history      

18/03/21
 * Ran optuna for HPO on RFC. However didn't use cross-validation score just split the data once re-used with no validation set.
   This led to leaking data from the test set to the model via improving the hyperparameters specifically for that test set. 
   Therefore, used cross-validation scores only going forward when running HPO and scoring the optimized models.
   
 * Speak about limitation above and how I'd use cross_val_score with more folds to achieve more consistent results (i.e. smaller sd)

19/03/21
 * Realised I was using regular k-folds in original algorithm test - should use stratified due to huge class imbalance.

22/03/21
 * OMG I have made so many mistakes,
 * So the RFC I thought was done turns out it was actually putting EVERYTHING in the same class (2) which happens to give an accuracy of 0.66
 * Started using classification_report from sklearn to see more advanced metrics and wow, XGBClassifier is really good and actually has class diversity
 * Gonna have to throw away everything from my report and likely start over
 * Read: https://stats.stackexchange.com/questions/392115/why-is-cross-val-score-substantially-lower-than-score-or-roc-auc-score/392135
   -> basically, stratified kfold doesnt shuffle the folds by default leading to some extreme bias as the data is sorted
   -> by shuffling the folds this bias is eliminated
   -> then when hyperparam tuning it doesn't gravitate towards parameters that put everything in one class (the most common class) and not improving
   -> this isn't by bad luck but a systematically wrong way of scoring the hyperparams (without shuffling the k folds) meaning it can never find parameters 
      that beat this 'cheaty' method
  
24/04/21
 * Been reading a lot about measuring metrics, recall vs precision etc.
 * For this dataset I think recall is more important as predicting someone will live when they will die is very very bad
 * The confusion matrix of RFC shows this very well with the model getting confused about true deceased vs predicted hospitalized/recovered